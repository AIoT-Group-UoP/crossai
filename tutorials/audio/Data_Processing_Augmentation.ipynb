{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1> Audio Data Augmentation example <h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "from crossai.loader import audio_loader"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load configuration file and dataset path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"../config.yml\", \"r\") as stream:\n",
    "    try:\n",
    "        config = yaml.safe_load(stream)\n",
    "    except yaml.YAMLError as exc:\n",
    "        print(exc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loaded data into the dataframe: 100%|██████████| 10/10 [00:07<00:00,  1.41it/s]                                                   \n"
     ]
    }
   ],
   "source": [
    "multi_dir_path = config['PATH']['RAW']['MINI_SPEECH_COMMANDS'] # store to dataframe\n",
    "# All signals are resampled at 22050Hz and normalised (0, 1)\n",
    "df = audio_loader(multi_dir_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create crossai audio data object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossai.pipelines.audio import Audio # , Finalize_Pipeline, Transformer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       [0.5122615, 0.51215744, 0.5122032, 0.5121585, ...\n",
       "1       [0.53833336, 0.5382485, 0.53829324, 0.53825945...\n",
       "2       [0.59909344, 0.5995758, 0.60014033, 0.5998662,...\n",
       "3       [0.46179047, 0.45909047, 0.45150578, 0.4502450...\n",
       "4       [0.5509648, 0.5498947, 0.549121, 0.54910535, 0...\n",
       "                              ...                        \n",
       "7995    [0.5364869, 0.5371126, 0.5387156, 0.5392515, 0...\n",
       "7996    [0.5423568, 0.5426123, 0.5428177, 0.5423651, 0...\n",
       "7997    [0.49612916, 0.49653575, 0.4966421, 0.4965381,...\n",
       "7998    [0.4939985, 0.49412805, 0.4942774, 0.49421456,...\n",
       "7999    [0.5626292, 0.5625252, 0.56263936, 0.5625625, ...\n",
       "Name: data, Length: 8000, dtype: object"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create crossai audio object\n",
    "cai_audio = Audio(df)\n",
    "cai_audio.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are contained inside audio_object.labels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    left\n",
       "1    left\n",
       "2    left\n",
       "3    left\n",
       "4    left\n",
       "Name: label, dtype: object"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cai_audio.labels.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create augmentations and augmentation order list."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossai.processing.augment import roll_signal\n",
    "from crossai.processing.audio.augment_audio import loudness\n",
    "from crossai.pipelines.preparation import Augmenter\n",
    "from crossai.pipelines.preparation import augment_signal_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "roll_aug = Augmenter(roll_signal, augment_times=1, n_roll=randint(10000, 50000))\n",
    "loudness_aug = Augmenter(loudness, augment_times=1, factor_low=4, factor_high=8)\n",
    "Augmentations = [roll_aug, loudness_aug]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Augment data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cai_audio_augmented = augment_signal_data(cai_audio, Augmentations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        [0.5122615, 0.51215744, 0.5122032, 0.5121585, ...\n",
       "1        [0.53833336, 0.5382485, 0.53829324, 0.53825945...\n",
       "2        [0.59909344, 0.5995758, 0.60014033, 0.5998662,...\n",
       "3        [0.46179047, 0.45909047, 0.45150578, 0.4502450...\n",
       "4        [0.5509648, 0.5498947, 0.549121, 0.54910535, 0...\n",
       "                               ...                        \n",
       "23995    [0.5364869, 0.5371126, 0.5387156, 0.5392515, 0...\n",
       "23996    [0.5423568, 0.5426123, 0.5428177, 0.5423651, 0...\n",
       "23997    [0.49612916, 0.49653575, 0.4966421, 0.4965381,...\n",
       "23998    [0.4939985, 0.49412805, 0.4942774, 0.49421456,...\n",
       "23999    [0.5626292, 0.5625252, 0.56263936, 0.5625625, ...\n",
       "Name: data, Length: 24000, dtype: object"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Augmented Data Lengh: 24000\n",
    "#   8000 (original) signals\n",
    "# + 8000 rolled signals (Rolling has been applied for each signal)\n",
    "# + 8000 loudness changed signals (Loudnes change has been applied for each signal)\n",
    "# --------\n",
    "# 24000 samples\n",
    "cai_audio_augmented.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The labels are updated aswell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        left\n",
       "1        left\n",
       "2        left\n",
       "3        left\n",
       "4        left\n",
       "         ... \n",
       "23995      no\n",
       "23996      no\n",
       "23997      no\n",
       "23998      no\n",
       "23999      no\n",
       "Name: label, Length: 24000, dtype: object"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cai_audio_augmented.labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1>Insertion into processing pipeline example <h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create an audio data to mel spectrograml transformer using create_transformer method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from crossai.processing.audio import melspectrogram\n",
    "from crossai.pipelines.timeseries import Transformer\n",
    "\n",
    "Spectrogram = Transformer(melspectrogram, sr=22050)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Create a processing pipeline that transforms the data into mel spectrograms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.pipeline import Pipeline\n",
    "# from crossai.pipelines.timeseries import ToPandas\n",
    "\n",
    "# pipe = Pipeline([('Spectrogram', Spectrogram), ('Finalize_Pipeline', ToPandas())])\n",
    "# pipe.fit_transform(cai_audio_augmented)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import librosa.display\n",
    "plt.figure(figsize=(14, 6))\n",
    "librosa.display.specshow(cai_audio_augmented.data[0], x_axis='time', y_axis='mel', fmax=8000)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2D Augmentation - SpecAugment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 2, got 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m/home/konto/crossai-playground/tutorials/audio/Data_Processing_Augmentation.ipynb Cell 27\u001b[0m line \u001b[0;36m5\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdgx.ceid.upatras.gr/home/konto/crossai-playground/tutorials/audio/Data_Processing_Augmentation.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=1'>2</a>\u001b[0m spec \u001b[39m=\u001b[39m Augmenter(spec_augment, augment_times\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, masks\u001b[39m=\u001b[39m\u001b[39m2\u001b[39m, freq_masking\u001b[39m=\u001b[39m\u001b[39m0.15\u001b[39m, time_masking\u001b[39m=\u001b[39m\u001b[39m0.15\u001b[39m)\n\u001b[1;32m      <a href='vscode-notebook-cell://ssh-remote%2Bdgx.ceid.upatras.gr/home/konto/crossai-playground/tutorials/audio/Data_Processing_Augmentation.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=2'>3</a>\u001b[0m Augmentations2D \u001b[39m=\u001b[39m [spec]\n\u001b[0;32m----> <a href='vscode-notebook-cell://ssh-remote%2Bdgx.ceid.upatras.gr/home/konto/crossai-playground/tutorials/audio/Data_Processing_Augmentation.ipynb#X35sdnNjb2RlLXJlbW90ZQ%3D%3D?line=4'>5</a>\u001b[0m cai_audio_spec_augmentned \u001b[39m=\u001b[39m augment_signal_data(cai_audio, Augmentations2D)\n",
      "File \u001b[0;32m~/anaconda3/envs/cai/lib/python3.11/site-packages/crossai/pipelines/preparation.py:54\u001b[0m, in \u001b[0;36maugment_signal_data\u001b[0;34m(crossai_object, augs)\u001b[0m\n\u001b[1;32m     51\u001b[0m df_aug[\u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m] \u001b[39m=\u001b[39m crossai_object\u001b[39m.\u001b[39mlabels\n\u001b[1;32m     53\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(augs)):\n\u001b[0;32m---> 54\u001b[0m     x \u001b[39m=\u001b[39m augs[i]\u001b[39m.\u001b[39;49mfit_transform(crossai_object)\n\u001b[1;32m     55\u001b[0m     df_aug \u001b[39m=\u001b[39m pd\u001b[39m.\u001b[39mconcat([df_aug, pd\u001b[39m.\u001b[39mDataFrame(\n\u001b[1;32m     56\u001b[0m         x, columns\u001b[39m=\u001b[39m[\u001b[39m'\u001b[39m\u001b[39mdata\u001b[39m\u001b[39m'\u001b[39m, \u001b[39m'\u001b[39m\u001b[39mlabel\u001b[39m\u001b[39m'\u001b[39m])], ignore_index\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m)\n\u001b[1;32m     58\u001b[0m crossai_object \u001b[39m=\u001b[39m TimeSeries(df_aug)\n",
      "File \u001b[0;32m~/anaconda3/envs/cai/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/cai/lib/python3.11/site-packages/sklearn/base.py:915\u001b[0m, in \u001b[0;36mTransformerMixin.fit_transform\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    911\u001b[0m \u001b[39m# non-optimized default implementation; override when a better\u001b[39;00m\n\u001b[1;32m    912\u001b[0m \u001b[39m# method is possible for a given clustering algorithm\u001b[39;00m\n\u001b[1;32m    913\u001b[0m \u001b[39mif\u001b[39;00m y \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[1;32m    914\u001b[0m     \u001b[39m# fit method of arity 1 (unsupervised transformation)\u001b[39;00m\n\u001b[0;32m--> 915\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfit(X, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mfit_params)\u001b[39m.\u001b[39;49mtransform(X)\n\u001b[1;32m    916\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    917\u001b[0m     \u001b[39m# fit method of arity 2 (supervised transformation)\u001b[39;00m\n\u001b[1;32m    918\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfit(X, y, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mfit_params)\u001b[39m.\u001b[39mtransform(X)\n",
      "File \u001b[0;32m~/anaconda3/envs/cai/lib/python3.11/site-packages/sklearn/utils/_set_output.py:140\u001b[0m, in \u001b[0;36m_wrap_method_output.<locals>.wrapped\u001b[0;34m(self, X, *args, **kwargs)\u001b[0m\n\u001b[1;32m    138\u001b[0m \u001b[39m@wraps\u001b[39m(f)\n\u001b[1;32m    139\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mwrapped\u001b[39m(\u001b[39mself\u001b[39m, X, \u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m--> 140\u001b[0m     data_to_wrap \u001b[39m=\u001b[39m f(\u001b[39mself\u001b[39;49m, X, \u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m    141\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(data_to_wrap, \u001b[39mtuple\u001b[39m):\n\u001b[1;32m    142\u001b[0m         \u001b[39m# only wrap the first output for cross decomposition\u001b[39;00m\n\u001b[1;32m    143\u001b[0m         return_tuple \u001b[39m=\u001b[39m (\n\u001b[1;32m    144\u001b[0m             _wrap_data_with_container(method, data_to_wrap[\u001b[39m0\u001b[39m], X, \u001b[39mself\u001b[39m),\n\u001b[1;32m    145\u001b[0m             \u001b[39m*\u001b[39mdata_to_wrap[\u001b[39m1\u001b[39m:],\n\u001b[1;32m    146\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/cai/lib/python3.11/site-packages/crossai/pipelines/preparation.py:29\u001b[0m, in \u001b[0;36mAugmenter.transform\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     26\u001b[0m data \u001b[39m=\u001b[39m []\n\u001b[1;32m     28\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(X\u001b[39m.\u001b[39mdata)):\n\u001b[0;32m---> 29\u001b[0m     x \u001b[39m=\u001b[39m [\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfunc(X\u001b[39m.\u001b[39;49mdata[i], \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mkwargs), X\u001b[39m.\u001b[39mlabels[i]]\n\u001b[1;32m     30\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(x[\u001b[39m0\u001b[39m]) \u001b[39m>\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m     31\u001b[0m         \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(\u001b[39mlen\u001b[39m(x[\u001b[39m0\u001b[39m])):\n",
      "File \u001b[0;32m~/anaconda3/envs/cai/lib/python3.11/site-packages/crossai/processing/augment.py:55\u001b[0m, in \u001b[0;36mspec_augment\u001b[0;34m(spectrogram, augment_times, masks, freq_masking, time_masking)\u001b[0m\n\u001b[1;32m     50\u001b[0m augmented \u001b[39m=\u001b[39m original_spectrogram\u001b[39m.\u001b[39mcopy()\n\u001b[1;32m     52\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(masks):\n\u001b[1;32m     53\u001b[0m \n\u001b[1;32m     54\u001b[0m     \u001b[39m# frequency masking\u001b[39;00m\n\u001b[0;32m---> 55\u001b[0m     freqs, time_frames \u001b[39m=\u001b[39m augmented\u001b[39m.\u001b[39mshape\n\u001b[1;32m     56\u001b[0m     freq_mask_percentage \u001b[39m=\u001b[39m random\u001b[39m.\u001b[39muniform(\u001b[39m0.0\u001b[39m, freq_masking)\n\u001b[1;32m     57\u001b[0m     masked_freqs \u001b[39m=\u001b[39m \u001b[39mint\u001b[39m(freq_mask_percentage \u001b[39m*\u001b[39m freqs)\n",
      "\u001b[0;31mValueError\u001b[0m: not enough values to unpack (expected 2, got 1)"
     ]
    }
   ],
   "source": [
    "from crossai.processing.augment import spec_augment\n",
    "spec = Augmenter(spec_augment, augment_times=1, masks=2, freq_masking=0.15, time_masking=0.15)\n",
    "Augmentations2D = [spec]\n",
    "\n",
    "cai_audio_spec_augmentned = augment_signal_data(cai_audio, Augmentations2D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "crossai_object.data"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
